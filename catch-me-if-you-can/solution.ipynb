{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import calmap\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit, cross_val_score, GridSearchCV, ParameterGrid\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display_html\n",
    "from category_encoders import WOEEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data/'\n",
    "SEED = 17\n",
    "TIMES = ['time%s' % i for i in range(1, 11)]\n",
    "SITES = ['site%s' % i for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'site_dic.pkl'), 'rb') as f:\n",
    "    site2id = pickle.load(f)\n",
    "id2site = {v:k for (k, v) in site2id.items()}\n",
    "id2site[0] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_vectorizer_params(params, train_df, test_df, idx_split, additional_data_df):\n",
    "    feature_names = list(additional_data_df.columns)\n",
    "    time_split = TimeSeriesSplit(n_splits=10)\n",
    "    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n",
    "    result = []\n",
    "\n",
    "    for vectorizer_params in ParameterGrid(params):\n",
    "        X_train, X_test, y_train, vectorizer = prepare_sparse_features(\n",
    "            train_df, test_df, vectorizer_params=vectorizer_params\n",
    "        )\n",
    "        X_train = hstack([X_train, additional_data_df.values[:idx_split,:]])\n",
    "        X_test = hstack([X_test, additional_data_df.values[idx_split:,:]])\n",
    "        \n",
    "        cv_scores = cross_val_score(\n",
    "            logit,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=time_split, \n",
    "            scoring='roc_auc',\n",
    "            n_jobs=4\n",
    "        )\n",
    "        result.append((cv_scores, vectorizer_params))\n",
    "        print(vectorizer_params)\n",
    "        print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n",
    "        print()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sites(sites):\n",
    "    result = []\n",
    "    for site_id in sites:\n",
    "        if site_id == 0:\n",
    "            continue\n",
    "        site = id2site[site_id]\n",
    "        if site.startswith('www.'):\n",
    "            site = site.replace('www.', '')\n",
    "        result.append(site)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file='submission.csv',\n",
    "                             target='target', index_label='session_id'):\n",
    "    predicted_df = pd.DataFrame(\n",
    "        predicted_labels,\n",
    "        index=np.arange(1, predicted_labels.shape[0] + 1),\n",
    "        columns=[target]\n",
    "    )\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes(path_to_train, path_to_test):\n",
    "    train_df = pd.read_csv(\n",
    "        path_to_train,\n",
    "        index_col='session_id',\n",
    "        parse_dates=TIMES\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        path_to_test,\n",
    "        index_col='session_id',\n",
    "        parse_dates=TIMES\n",
    "    )\n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_features(train_df, test_df, vectorizer_params):\n",
    "\n",
    "    idx_split = train_df.shape[0]\n",
    "    full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "    sessions = full_df[SITES].fillna(0).astype('int').apply(\n",
    "        lambda row: _get_sites(row), axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "    vectorized_sessions = vectorizer.fit_transform(sessions)\n",
    "\n",
    "    X_train = vectorized_sessions[:idx_split,:]\n",
    "    X_test = vectorized_sessions[idx_split:,:]\n",
    "    y_train = train_df['target'].astype('int').values\n",
    "\n",
    "    return X_train, X_test, y_train, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(model, X_train, y_train, X_test, site_feature_names,\n",
    "                      cv, new_feature_names=None, submission_file_name=None):\n",
    "    cv_scores = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4\n",
    "    )\n",
    "    print('CV scores', cv_scores)\n",
    "    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if new_feature_names:\n",
    "        all_feature_names = site_feature_names + new_feature_names\n",
    "    else: \n",
    "        all_feature_names = site_feature_names\n",
    "        \n",
    "    if new_feature_names:\n",
    "        print('New feature weights:')\n",
    "        print(pd.DataFrame({\n",
    "            'feature': new_feature_names, \n",
    "            'coef': model.coef_.flatten()[-len(new_feature_names):]\n",
    "        }))\n",
    "\n",
    "    if submission_file_name:\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        write_to_submission_file(test_pred, submission_file_name) \n",
    "        \n",
    "    display_html(eli5.show_weights(\n",
    "        estimator=model,\n",
    "        feature_names=all_feature_names,\n",
    "        top=30\n",
    "    ))\n",
    "\n",
    "    return cv_scores, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем с помощью `TfidfVectorizer` разряженные матрицы на основе датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_dataframes(\n",
    "    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = train_df.shape[0]\n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "y_train = train_df['target']\n",
    "additional_data_df = pd.DataFrame(index=full_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим **время суток** как WOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hour_train_df = pd.DataFrame(index=train_df.index)\n",
    "start_hour_train_df['start_hour'] = train_df['time1'].dt.hour\n",
    "\n",
    "start_hour_test_df = pd.DataFrame(index=test_df.index)\n",
    "start_hour_test_df['start_hour'] = test_df['time1'].dt.hour\n",
    "\n",
    "woe_enc = WOEEncoder(cols=['start_hour'], random_state=SEED)\n",
    "woe_enc.fit(start_hour_train_df, y_train)\n",
    "\n",
    "start_hour_train_df['start_hour'] = woe_enc.transform(start_hour_train_df)\n",
    "start_hour_test_df['start_hour'] = woe_enc.transform(start_hour_test_df)\n",
    "\n",
    "start_hour_full_df = pd.concat([start_hour_train_df, start_hour_test_df])\n",
    "additional_data_df = pd.concat([additional_data_df, start_hour_full_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим **продолжительность сессии**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df['duration'] = (\n",
    "    full_df[TIMES].max(axis=1) - full_df[TIMES].min(axis=1)\n",
    ").astype('timedelta64[ms]').astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "additional_data_df['duration'] = scaler.fit_transform(\n",
    "    additional_data_df['duration'].values.reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим **сезон**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df['summer'] = (full_df['time1'].dt.month.isin([6, 7, 8])).astype('int')\n",
    "additional_data_df['autumn'] = (full_df['time1'].dt.month.isin([9, 10, 11])).astype('int')\n",
    "additional_data_df['winter'] = (full_df['time1'].dt.month.isin([12, 1, 2])).astype('int')\n",
    "additional_data_df['spring'] = (full_df['time1'].dt.month.isin([3, 4, 5])).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим **день недели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df['day_of_week'] = full_df['time1'].apply(\n",
    "    lambda t: t.weekday()).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим логарифм от числа **год + месяц**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df['year_month'] = np.log(full_df['time1'].apply(\n",
    "    lambda t: 100 * t.year + t.month).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закончилась ли сессия **просмотром 10 сайтов**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df['site10'] = full_df['site10'].isna().astype('int').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим **номер недели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_week_train_df = pd.DataFrame(index=train_df.index)\n",
    "n_week_train_df['n_week'] = train_df['time1'].dt.week\n",
    "\n",
    "n_week_test_df = pd.DataFrame(index=test_df.index)\n",
    "n_week_test_df['n_week'] = test_df['time1'].dt.week\n",
    "\n",
    "woe_enc = WOEEncoder(cols=['n_week'], random_state=SEED)\n",
    "woe_enc.fit(n_week_train_df, y_train)\n",
    "\n",
    "n_week_train_df['n_week'] = woe_enc.transform(n_week_train_df)\n",
    "n_week_test_df['n_week'] = woe_enc.transform(n_week_test_df)\n",
    "\n",
    "n_week_full_df = pd.concat([n_week_train_df, n_week_test_df])\n",
    "additional_data_df = pd.concat([additional_data_df, n_week_full_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site1          0.001635\n",
       "site2          0.003593\n",
       "site3          0.004641\n",
       "site4          0.003328\n",
       "site5          0.005701\n",
       "site6          0.004205\n",
       "site7          0.002898\n",
       "site8          0.006590\n",
       "site9          0.004619\n",
       "site10         0.006350\n",
       "target         1.000000\n",
       "start_hour     0.136587\n",
       "duration       0.027864\n",
       "summer         0.009585\n",
       "autumn         0.051446\n",
       "winter         0.023724\n",
       "spring         0.012645\n",
       "day_of_week    0.041859\n",
       "year_month     0.034096\n",
       "site10         0.024135\n",
       "n_week         0.087268\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.concat([train_df, additional_data_df[:idx_split]], axis=1)\n",
    "corrs = tmp.corr()['target'].abs()\n",
    "corrs[corrs>.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {\n",
    "    'max_df': 0.25,\n",
    "    'ngram_range': (1, 6),\n",
    "    'max_features': 100000,\n",
    "    'norm': 'l2',\n",
    "    'binary': True,\n",
    "    'tokenizer': lambda s: s.split()\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, vectorizer = prepare_sparse_features(\n",
    "    train_df, test_df, vectorizer_params=vectorizer_params\n",
    ")\n",
    "\n",
    "X_train = hstack([X_train, additional_data_df.values[:idx_split,:]])\n",
    "X_test = hstack([X_test, additional_data_df.values[idx_split:,:]])\n",
    "feature_names = list(additional_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores [0.86643123 0.91419303 0.95825852 0.95338674 0.95789663 0.98008842\n",
      " 0.92195744 0.98545972 0.95585084 0.99093732]\n",
      "CV mean: 0.9484459881980023, CV std: 0.03611501747527895\n"
     ]
    }
   ],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "logit_params = {\n",
    "    'C': 1.623776739188721,\n",
    "    'penalty': 'l2',\n",
    "    'solver': 'liblinear',\n",
    "    'random_state': SEED\n",
    "}\n",
    "logit = LogisticRegression(**logit_params)\n",
    "\n",
    "cv_score = train_and_predict(\n",
    "    model=logit,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    site_feature_names=vectorizer.get_feature_names(),\n",
    "    new_feature_names=feature_names,\n",
    "    cv=time_split,\n",
    "    submission_file_name='submission.csv'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     'C': np.logspace(-4, 4, 20)\n",
    "# }\n",
    "\n",
    "# logit_text_clf = GridSearchCV(logit, parameters, cv=time_split, scoring='roc_auc')\n",
    "# logit_text_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = train_df[train_df['target'] == 1]\n",
    "\n",
    "alice_sessions = (\n",
    "    alice['time1'].dt.round('D')\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "plt.scatter(alice_sessions.index, alice_sessions.values)\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('number of sessions')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_alice = train_df[train_df['target'] == 0]\n",
    "\n",
    "not_alice_sessions = (\n",
    "    not_alice['time1'].dt.round('D')\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "plt.scatter(not_alice_sessions.index, not_alice_sessions.values)\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('number of sessions')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions = (\n",
    "    test_df['time1'].dt.round('D')\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "plt.scatter(test_sessions.index, test_sessions.values)\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('number of sessions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calmap.yearplot(test_sessions, year=2014);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calmap.yearplot(alice_sessions, year=2013);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calmap.yearplot(not_alice_sessions, year=2013);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calmap.yearplot(alice_sessions, year=2014);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calmap.yearplot(not_alice_sessions, year=2014);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
